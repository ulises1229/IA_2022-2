{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unidad 3 - Introducción  Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulises1229/IA-2021-II/blob/main/code/Unidad_3_Introducci%C3%B3n_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAc5bE406eF3"
      },
      "source": [
        "# Inteligencia artificial 2021-2 \n",
        "## UNIDAD 3 - Introducción a Deep Learning\n",
        "+ 15/04/2021\n",
        "\n",
        "#### Código fuente y contenido basado en: \n",
        "`MIT 6.S191: Introduction to Deep Learning`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBk0ZDWY-ff8"
      },
      "source": [
        "<table align=\"center\">\n",
        "\n",
        "<td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/drive/1v15ZcgIdM9eARdqYuMPfy8boHlLgU6vp?usp=sharing\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Ejecutar en Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ulises1229/IA-2021-II/blob/main/code/Unidad_3_Introducci%C3%B3n_Deep_Learning.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />Ver código fuente en GitHub</a></td>\n",
        "\n",
        "\n",
        "</table>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "# 1. Introducción a TensorFlow y RNN\n",
        "## 1.1 Instalación de TensorFlow\n",
        "\n",
        "TensorFlow es una framework de software muy utilizado en el aprendizaje automático. En esta sección se comprenderá cómo se representan los cálculos y cómo definir una red neuronal simple en TensorFlow. Usaremos la última versión de TensorFlow, TensorFlow 2, que ofrece una gran flexibilidad y la capacidad de realizar operaciones de manera imperativa, al igual que en Python. Notarás que TensorFlow 2 es bastante similar a Python en su sintaxis y ejecución imperativa.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkaimNJfYZ2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ab3386-e538-4109-e66e-352d850bfbb1"
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download and import the MIT 6.S191 package\n",
        "#!pip install mitdeeplearning\n",
        "#import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNMcdP4m3Vs"
      },
      "source": [
        "## 1.1 ¿Porqué TensorFlow se llama Tensor Flow?\n",
        "\n",
        "TensorFlow se llama 'TensorFlow' porque maneja el flujo (nodo / operación matemática) de Tensores, que son estructuras de datos que puede considerar como matrices multidimensionales. Los tensores se representan como matrices n-dimensionales de tipos de datos base, como una cadena o un entero; proporcionan una forma de generalizar vectores y matrices a dimensiones más altas.\n",
        "\n",
        "La ``` forma``` de un tensor define su número de dimensiones y el tamaño de cada dimensión. El ``` rango ``` de un tensor proporciona el número de dimensiones (n-dimensiones); también puede pensar en esto como el orden o grado del tensor.\n",
        "\n",
        "Veamos primero los tensores 0-d, de los cuales un escalar es un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFxztZQInlAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d403c5-875e-420d-a545-4283171bebcb"
      },
      "source": [
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "number = tf.constant(1.41421356237, tf.float64)\n",
        "\n",
        "print(\"`sport` is a {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
        "print(\"`number` is a {}-d Tensor\".format(tf.rank(number).numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`sport` is a 0-d Tensor\n",
            "`number` is a 0-d Tensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dljcPUcoJZ6"
      },
      "source": [
        "*Los vectores* y las listas se pueden utilizar para crear tensores 1-d:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaHXABe8oPcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1730bb-7df8-4462-9146-beb98ada0ad7"
      },
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
        "\n",
        "print(\"`sports` is a {}-d Tensor with shape: {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers).numpy(), tf.shape(numbers)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`sports` is a 1-d Tensor with shape: [2]\n",
            "`numbers` is a 1-d Tensor with shape: [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvffwkvtodLP"
      },
      "source": [
        "A continuación, consideramos la creación de 2-d (es decir, matrices) y tensores de rango superior. Por ejemplo, en futuras practicas que involucren procesamiento de imágenes y visión por computadora, usaremos tensores 4-d. Aquí las dimensiones corresponden a la cantidad de imágenes de ejemplo en nuestro lote, la altura de la imagen, el ancho de la imagen y la cantidad de canales de color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFeBBe1IouS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "1e4ceee7-e2c8-4806-c3fa-3ed438260daa"
      },
      "source": [
        "### Defining higher-order Tensors ###\n",
        "\n",
        "'''TODO: Define a 2-d Tensor'''\n",
        "matrix = # TODO\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(matrix).numpy() == 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-22ce4a433b08>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    matrix = # TODO\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv1fTn_Ya_cz"
      },
      "source": [
        "'''TODO: Define a 4-d Tensor.'''\n",
        "# Use tf.zeros to initialize a 4-d Tensor of zeros with size 10 x 256 x 256 x 3. \n",
        "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
        "images = # TODO\n",
        "\n",
        "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaCDOGapMyl"
      },
      "source": [
        "Como ha visto, la ``forma`` de un tensor proporciona el número de elementos en cada dimensión del tensor. La ``forma`` es bastante útil y la usaremos a menudo. También puede usar la división para acceder a los subtensores dentro de un tensor de rango superior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhaufyObuLEG"
      },
      "source": [
        "row_vector = matrix[1]\n",
        "column_vector = matrix[:,2]\n",
        "scalar = matrix[1, 2]\n",
        "\n",
        "print(\"`row_vector`: {}\".format(row_vector.numpy()))\n",
        "print(\"`column_vector`: {}\".format(column_vector.numpy()))\n",
        "print(\"`scalar`: {}\".format(scalar.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VO-LZYZ2z"
      },
      "source": [
        "## 1.2 Cálculos sobre tensores\n",
        "\n",
        "Una forma conveniente de pensar y visualizar los cálculos en TensorFlow es en términos de gráficos. Podemos definir este gráfico en términos de tensores, que contienen datos, y las operaciones matemáticas que actúan sobre estos tensores en algún orden. Veamos un ejemplo simple y definamos este cálculo usando TensorFlow:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/add-graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_YJrZsxYZ2z"
      },
      "source": [
        "# Create the nodes in the graph, and initialize values\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "\n",
        "# Add them!\n",
        "c1 = tf.add(a,b)\n",
        "c2 = a + b # TensorFlow overrides the \"+\" operation so that it is able to act on Tensors\n",
        "print(c1)\n",
        "print(c2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbfv_QOiYZ23"
      },
      "source": [
        "Observe cómo hemos creado un gráfico de cálculo que consta de operaciones de TensorFlow y cómo la salida es un tensor con valor 76; acabamos de crear un gráfico de cálculo que consta de operaciones, las ejecuta y nos devuelve el resultado.\n",
        "\n",
        "Ahora consideremos un ejemplo un poco más complicado:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph.png)\n",
        "\n",
        "Aquí, tomamos dos entradas, `a,b`, y calculamos una salida `e`. Cada nodo en el gráfico representa una operación que toma alguna entrada, realiza algunos cálculos y pasa su salida a otro nodo.\n",
        "\n",
        "Definamos una función simple en TensorFlow para construir esta función de cálculo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PJnfzpWyYZ23"
      },
      "source": [
        "### Defining Tensor computations ###\n",
        "\n",
        "# Construct a simple computation function\n",
        "def func(a,b):\n",
        "  '''TODO: Define the operation for c, d, e (use tf.add, tf.subtract, tf.multiply).'''\n",
        "  c = # TODO\n",
        "  d = # TODO\n",
        "  e = # TODO\n",
        "  return e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrRfDMS2-oy"
      },
      "source": [
        "Ahora, podemos llamar a esta función para ejecutar el gráfico de cálculo dadas algunas entradas `a, b`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnwsf8w2uF7p"
      },
      "source": [
        "# Consider example values for a,b\n",
        "a, b = 1.5, 2.5\n",
        "# Execute the computation\n",
        "e_out = func(a,b)\n",
        "print(e_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqgUIUhYZ29"
      },
      "source": [
        "Observe cómo nuestra salida es un tensor con un valor definido por la salida del cálculo, y que la salida no tiene forma, ya que es un valor escalar único."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h4o9Bb0YZ29"
      },
      "source": [
        "## 1.3 Redes Neuronales en TensorFlow\n",
        "También podemos definir redes neuronales en TensorFlow. TensorFlow utiliza una API de alto nivel llamada [Keras] (https://www.tensorflow.org/guide/keras) que proporciona un marco potente e intuitivo para crear y entrenar modelos de aprendizaje profundo.\n",
        "\n",
        "Consideremos primero el ejemplo de un perceptrón simple definido por una sola capa densa: $ y = \\ sigma (Wx + b) $, donde $ W $ representa una matriz de pesos, $ b $ es un sesgo, $ x $ es el input, $ \\ sigma $ es la función de activación sigmoidea, y $ y $ es la salida. También podemos ver esta operación usando un gráfico:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph-2.png)\n",
        "\n",
        "Los tensores pueden fluir a través de tipos abstractos llamados [```Layers```] (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer), los componentes básicos de las redes neuronales. Las ``capas`` implementan operaciones de redes neuronales comunes y se utilizan para actualizar pesos, calcular pérdidas y definir la conectividad entre capas. Primero definiremos la ``Capa`` para implementar el perceptrón simple definido anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HutbJk-1kHPh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "8063bec3-93f8-40ac-e07d-ae39b8c8c4e3"
      },
      "source": [
        "### Defining a network Layer ###\n",
        "# n_output_nodes: number of output nodes\n",
        "# input_shape: shape of the input\n",
        "# x: input to the layer\n",
        "\n",
        "class OurDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(OurDenseLayer, self).__init__()\n",
        "    self.n_output_nodes = n_output_nodes\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    d = int(input_shape[-1])\n",
        "    # Define and initialize parameters: a weight matrix W and bias b\n",
        "    # Note that parameter initialization is random!\n",
        "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # note the dimensionality\n",
        "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # note the dimensionality\n",
        "\n",
        "  def call(self, x):\n",
        "    '''TODO: define the operation for z (hint: use tf.matmul)'''\n",
        "    z = # TODO\n",
        "\n",
        "    '''TODO: define the operation for out (hint: use tf.sigmoid)'''\n",
        "    y = # TODO\n",
        "    return y\n",
        "\n",
        "# Since layer parameters are initialized randomly, we will set a random seed for reproducibility\n",
        "tf.random.set_seed(1)\n",
        "layer = OurDenseLayer(3)\n",
        "layer.build((1,2))\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "# test the output!\n",
        "print(y.numpy())\n",
        "mdl.lab1.test_custom_dense_layer_output(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a0a79e7441c5>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    z = # TODO\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt1FgM7qYZ3D"
      },
      "source": [
        "Convenientemente, TensorFlow ha definido una serie de `Capas` que se utilizan comúnmente en redes neuronales, por ejemplo, una [`Capa Densa`](https://www.tensorflow.org/api_docs/python/tf/keras/layer/Dense?version=estable). Ahora, en lugar de usar una sola `Capa` para definir nuestra red neuronal simple, usaremos la [`Secuencial`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) de Keras y una sola capa [`Densa`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) para definir nuestra red. Con la API `Secuencial`, puede crear fácilmente redes neuronales apilando capas como bloques de construcción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WXTpmoL6TDz"
      },
      "source": [
        "### Defining a neural network using the Sequential API ###\n",
        "\n",
        "# Import relevant packages\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the number of outputs\n",
        "n_output_nodes = 3\n",
        "\n",
        "# First define the model \n",
        "model = Sequential()\n",
        "\n",
        "'''TODO: Define a dense (fully connected) layer to compute z'''\n",
        "# Remember: dense layers are defined by the parameters W and b!\n",
        "# You can read more about the initialization of W and b in the TF documentation :) \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable\n",
        "dense_layer = # TODO\n",
        "\n",
        "# Add the dense layer to the model\n",
        "model.add(dense_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGcwYfUyR-U"
      },
      "source": [
        "Hemos definido nuestro modelo utilizando la API secuencial. Ahora, podemos probarlo usando una entrada de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg23OczByRDb"
      },
      "source": [
        "# Test model with example input\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "'''TODO: feed input into the model and predict the output!'''\n",
        "model_output = # TODO\n",
        "print(model_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596NvsOOtr9F"
      },
      "source": [
        "Además de definir modelos utilizando la API secuencial, también podemos definir redes neuronales subclasificando directamente la clase Model, que agrupa las capas para permitir el entrenamiento y la inferencia del modelo. La clase Model captura lo que llamamos \"modelo\" o \"red\". Usando Subclases, podemos crear una clase para nuestro modelo y luego definir el paso directo a través de la red usando la función de llamada. La subclasificación ofrece la flexibilidad de definir capas personalizadas, ciclos de entrenamiento personalizados, funciones de activación personalizadas y modelos personalizados. Definamos ahora la misma red neuronal anterior utilizando Subclases en lugar del modelo Secuencial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4aCflPVyViD"
      },
      "source": [
        "### Defining a model using subclassing ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "\n",
        "  # In __init__, we define the Model's layers\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(SubclassModel, self).__init__()\n",
        "    '''TODO: Our model consists of a single Dense layer. Define this layer.''' \n",
        "    self.dense_layer = '''TODO: Dense Layer'''\n",
        "\n",
        "  # In the call function, we define the Model's forward pass.\n",
        "  def call(self, inputs):\n",
        "    return self.dense_layer(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-lwHDk4irB"
      },
      "source": [
        "Al igual que el modelo que construimos usando la API `Sequential`, probemos nuestro` SubclassModel` usando una entrada de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhB34RA-4gXb"
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = SubclassModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model.call(x_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIFMJLAzsyE"
      },
      "source": [
        "Es importante destacar que la subclasificación nos brinda mucha flexibilidad para definir modelos personalizados. Por ejemplo, podemos usar argumentos booleanos en la función `call` para especificar diferentes comportamientos de red, por ejemplo, diferentes comportamientos durante el entrenamiento y la inferencia. Supongamos que en algunos casos queremos que nuestra red simplemente genere la entrada, sin ninguna perturbación. Definimos un argumento booleano `isidentity` para controlar este comportamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7jzGX5D1xT5"
      },
      "source": [
        "### Defining a model using subclassing and specifying custom behavior ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "\n",
        "  # As before, in __init__ we define the Model's layers\n",
        "  # Since our desired behavior involves the forward pass, this part is unchanged\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(IdentityModel, self).__init__()\n",
        "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
        "\n",
        "  '''TODO: Implement the behavior where the network outputs the input, unchanged, \n",
        "      under control of the isidentity argument.'''\n",
        "  def call(self, inputs, isidentity=False):\n",
        "    x = self.dense_layer(inputs)\n",
        "    '''TODO: Implement identity behavior'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku4rcCGx5T3y"
      },
      "source": [
        "Probemos este comportamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzC0mgbk5dp2"
      },
      "source": [
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "'''TODO: pass the input into the model and call with and without the input identity option.'''\n",
        "out_activate = # TODO\n",
        "out_identity = # TODO\n",
        "\n",
        "print(\"Network output with activation: {}; network identity output: {}\".format(out_activate.numpy(), out_identity.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1dEqdk6VI5"
      },
      "source": [
        "Ahora que hemos aprendido cómo definir `Capas` y redes neuronales en TensorFlow utilizando las API de `Secuencial` y de Subclases, estamos listos para centrar nuestra atención en cómo implementar realmente el entrenamiento de red con retropropagación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQwDhKn8kbO2"
      },
      "source": [
        "## 1.4 Diferenciación automática en TensorFlow\n",
        "\n",
        "[Diferenciación automática] (https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
        "es una de las partes más importantes de TensorFlow y es la columna vertebral del entrenamiento con\n",
        "[backpropagation] (https://en.wikipedia.org/wiki/Backpropagation). Usaremos TensorFlow GradientTape [`tf.GradientTape`] (https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable) para rastrear operaciones para calcular gradientes más adelante.\n",
        "\n",
        "Cuando se realiza un pase directo a través de la red, todas las operaciones de paso directo se graban en una \"cinta\"; luego, para calcular el gradiente, la cinta se reproduce al revés. De forma predeterminada, la cinta se descarta después de reproducirla al revés; esto significa que un `tf.GradientTape` en particular solo puede\n",
        "calcula un gradiente y las llamadas posteriores arrojan un error de tiempo de ejecución. Sin embargo, podemos calcular varios degradados sobre el mismo cálculo creando una cinta de degradado `` persistente ''.\n",
        "\n",
        "Primero, veremos cómo podemos calcular gradientes usando GradientTape y acceder a ellos para el cálculo. Definimos la función simple $ y = x ^ 2 $ y calculamos el gradiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkqk8pw5yJM"
      },
      "source": [
        "### Gradient computation with GradientTape ###\n",
        "\n",
        "# y = x^2\n",
        "# Example: x = 3.0\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Initiate the gradient tape\n",
        "with tf.GradientTape() as tape:\n",
        "  # Define the function\n",
        "  y = x * x\n",
        "# Access the gradient -- derivative of y with respect to x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "assert dy_dx.numpy() == 6.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhU5metS5xF3"
      },
      "source": [
        "En el entrenamiento de redes neuronales, utilizamos la diferenciación y el descenso de gradiente estocástico (SGD) para optimizar la función de pérdida. Ahora que tenemos una idea de cómo se puede usar `GradientTape` para calcular y acceder a las derivadas, veremos un ejemplo en el que usamos la diferenciación automática y SGD para encontrar el mínimo de $ L = (x-x_f) ^ 2 $. Aquí $ x_f $ es una variable para un valor deseado que estamos tratando de optimizar; $ L $ representa una pérdida que estamos tratando de minimizar. Si bien claramente podemos resolver este problema analíticamente ($ x_ {min} = x_f $), considerar cómo podemos calcular esto usando `GradientTape` nos prepara muy bien para futuros laboratorios donde usamos el descenso de gradiente para optimizar todas las pérdidas de la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "id": "7g1yWiSXqEf-"
      },
      "source": [
        "### Function minimization with automatic differentiation and SGD ###\n",
        "\n",
        "# Initialize a random value for our initial x\n",
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print(\"Initializing x={}\".format(x.numpy()))\n",
        "\n",
        "learning_rate = 1e-2 # learning rate for SGD\n",
        "history = []\n",
        "# Define the target value\n",
        "x_f = 4\n",
        "\n",
        "# We will run SGD for a number of iterations. At each iteration, we compute the loss, \n",
        "#   compute the derivative of the loss with respect to x, and perform the SGD update.\n",
        "for i in range(500):\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''TODO: define the loss as described above'''\n",
        "    loss = # TODO\n",
        "\n",
        "  # loss minimization using gradient tape\n",
        "  grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n",
        "  new_x = x - learning_rate*grad # sgd update\n",
        "  x.assign(new_x) # update the value of x\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "# Plot the evolution of x as we optimize towards x_f!\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500],[x_f,x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7czCwk3ceH"
      },
      "source": [
        "`GradientTape` proporciona un marco extremadamente flexible para la diferenciación automática. Para retroceder los errores de propagación a través de una red neuronal, realizamos un seguimiento de los pases hacia adelante en la cinta, usamos esta información para determinar los gradientes y luego usamos estos gradientes para la optimización mediante SGD."
      ]
    }
  ]
}